<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Monocular Robot Navigation with Self-Supervised Pretrained Vision
  Transformers</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }
        .container { background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .paper-info { background: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 20px; }
        .link { margin: 20px 0; text-align: center; }
        a { 
            color: #0066cc; 
            text-decoration: none; 
            background-color: #e3f2fd;
            padding: 10px 20px;
            border-radius: 5px;
            display: inline-block;
            transition: all 0.3s;
        }
        a:hover { 
            background-color: #bbdefb;
            transform: translateY(-2px);
        }
        h1 { color: #333; border-bottom: 2px solid #0066cc; padding-bottom: 10px; }
        .meta { color: #666; margin: 5px 0; }
        .abstract { background-color: #fafafa; padding: 15px; border-left: 4px solid #0066cc; margin: 15px 0; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“„ Monocular Robot Navigation with Self-Supervised Pretrained Vision
  Transformers</h1>
        
        <div class="paper-info">
            <div class="meta"><strong>ğŸ‘¥ ä½œè€…:</strong> Miguel Saavedra-Ruiz, Sacha Morin, Liam Paull</div>
            <div class="meta"><strong>ğŸ“… å‘å¸ƒæ—¥æœŸ:</strong> 2022-03-07</div>
            <div class="meta"><strong>ğŸ“š æ¥æº:</strong> arXiv</div>
            
            
        </div>
        
        <div class="abstract">
            <strong>ğŸ“ æ‘˜è¦:</strong><br>
            In this work, we consider the problem of learning a perception model for
monocular robot navigation using few annotated images. Using a Vision
Transformer (ViT) pretrained with a label-free self-supervised method, we
successfully train a coarse image segmentation model for the Duckietown
environment using 70 training images. Our model performs coarse image
segmentation at the 8x8 patch level, and the inference resolution can be
adjusted to balance prediction granularity and real-time perception
constraints. We study how best to adapt a ViT to our task and environment, and
find that some lightweight architectures can yield good single-image
segmentation at a usable frame rate, even on CPU. The resulting perception
model is used as the backbone for a simple yet robust visual servoing agent,
...
        </div>
        
        <div class="link">
            <h3>ğŸ”— è®¿é—®åŸæ–‡:</h3>
            <a href="http://arxiv.org/abs/2203.03682v2" target="_blank">
                ç‚¹å‡»è®¿é—®åŸæ–‡ â†’
            </a>
        </div>
        
        <div style="text-align: center; color: #888; font-size: 12px; margin-top: 30px;">
            <p>ğŸ“„ ç”±æœºå™¨è§†è§‰æ–‡çŒ®è·å–ç³»ç»Ÿç”Ÿæˆ</p>
            <p>ğŸ•’ ç”Ÿæˆæ—¶é—´: 2025-05-26 23:35:25</p>
        </div>
    </div>
</body>
</html>